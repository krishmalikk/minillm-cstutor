# Training Configuration for CS Tutor LLM
# Optimized for instruction-tuning on CS topics

# ============================================
# Training Hyperparameters
# ============================================
training:
  # Batch settings
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 8
  effective_batch_size: 32  # per_device * accumulation * num_gpus
  
  # Learning rate
  learning_rate: 3.0e-4
  min_learning_rate: 1.0e-5
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.03
  warmup_steps: 100
  
  # Training duration
  num_train_epochs: 3
  max_steps: -1  # -1 means use num_train_epochs
  
  # Optimizer
  optimizer: "adamw"
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1.0e-8
  weight_decay: 0.1
  max_grad_norm: 1.0
  
  # Mixed precision
  fp16: false
  bf16: true  # Use bf16 if available (A100, H100, M1/M2/M3)
  tf32: true  # Enable TF32 on Ampere+ GPUs
  
  # Memory optimization
  gradient_checkpointing: true
  use_flash_attention: true
  
  # Logging
  logging_steps: 10
  eval_steps: 500
  save_steps: 1000
  save_total_limit: 3
  
  # Evaluation
  evaluation_strategy: "steps"
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false

# ============================================
# Data Configuration
# ============================================
data:
  train_file: "data/train.jsonl"
  eval_file: "data/eval.jsonl"
  test_file: "data/test.jsonl"
  
  max_seq_length: 2048
  preprocessing_num_workers: 4
  
  # Prompt template
  prompt_template: |
    ### Instruction:
    {instruction}
    
    ### Input:
    {input}
    
    ### Response:
    {output}

# ============================================
# LoRA Configuration
# ============================================
lora:
  enabled: true
  r: 16  # LoRA rank
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  bias: "none"
  task_type: "CAUSAL_LM"

# ============================================
# Distributed Training
# ============================================
distributed:
  strategy: "ddp"  # ddp, fsdp, deepspeed
  num_gpus: 1
  
  # DeepSpeed config (if strategy == "deepspeed")
  deepspeed:
    stage: 2
    offload_optimizer: false
    offload_param: false

# ============================================
# Checkpointing
# ============================================
checkpointing:
  output_dir: "outputs/checkpoints"
  resume_from_checkpoint: null
  save_safetensors: true

# ============================================
# Wandb Logging (optional)
# ============================================
wandb:
  enabled: false
  project: "cs-tutor-llm"
  entity: null
  run_name: null

